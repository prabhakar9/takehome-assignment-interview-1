{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "distilbert_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhas-chowdary/20newsgroup/blob/main/distilbert_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbLIoYozAMG2"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import collections\n",
        "import timeit\n",
        "import torch\n",
        "import pandas as pd\n",
        "import pkbar\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import  f1_score,classification_report\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML4e3-8RDgun"
      },
      "source": [
        "# Uses GPU if available\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKMz-e_6DV_p"
      },
      "source": [
        "# If running on google colab: Uncomment below code and install transformers and pkbar libraries.\n",
        "\n",
        "# !pip install transformers\n",
        "# !pip install pkbar\n",
        "from transformers import DistilBertConfig,DistilBertTokenizer,DistilBertModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf5u6xPt_mgy"
      },
      "source": [
        "# If running on google colab:  Upload 'data.zip' file present in git repository to colab, uncomment below code and run it.\n",
        "# This mounts the data to google colab.\n",
        "# !unzip data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEe5qpei87tR"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VktKnxZXHLOL"
      },
      "source": [
        "# Path to documents.\n",
        "dir_path = os.getcwd()\n",
        "rel_path = \"data\"\n",
        "data_path = os.path.join(dir_path, rel_path)\n",
        "news_groups = [f for f in os.listdir(data_path)]\n",
        "news_group_idx = {v:i for i,v in enumerate(news_groups)}\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTAJO26p9G9H"
      },
      "source": [
        "# Preprocess data: Remove all special characters, convert to lower case.\n",
        "def data_preprocess(cur):\n",
        "        cur = cur.lower()\n",
        "        cur = re.sub(r'[\\w\\.-]+@[\\w\\.-]+',' ',cur)\n",
        "        cur = re.sub(\"[^a-zA-Z,.']\", ' ', cur)\n",
        "        cur = re.sub(r'\\.{2,}',' ',cur)   \n",
        "        cur = re.sub('\\s+',' ',cur)\n",
        "        cur = \" \".join(cur.split())\n",
        "        return cur"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGQz_VB3AZE2"
      },
      "source": [
        "# Prepare dataset: Read data from docs. \n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for category in news_groups:\n",
        "    cls = []\n",
        "    f_path = os.path.join(data_path,category)\n",
        "    # Read data from all docs.\n",
        "    for files in os.listdir(f_path):\n",
        "        path = os.path.join(f_path,files)\n",
        "        with open(path,'r',errors='ignore',encoding=\"utf8\") as file:\n",
        "            cur_doc = data_preprocess(file.read().replace('\\n',' '))\n",
        "            X.append(cur_doc)\n",
        "        y.append(int(news_group_idx[category]))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jq2JLUPfhCU",
        "outputId": "7af90aa7-a07a-437d-8972-52f2d53dfa0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# train test split\n",
        "X_train,X_test,y_train,y_test= train_test_split(X,y,stratify=y,test_size=0.2, \n",
        "                                                random_state=9)\n",
        "print('number of training samples:', len(X_train))\n",
        "print('number of test samples:', len(X_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training samples: 16333\n",
            "number of test samples: 4084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT8cIf2IcN1S"
      },
      "source": [
        "train_df = pd.DataFrame({'doc':X_train,\n",
        "                         'labels':y_train})\n",
        "test_df = pd.DataFrame({'doc':X_test,\n",
        "                         'labels':y_test})"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_sfKkF7emQk"
      },
      "source": [
        "# Distil-bert model parameters\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 8\n",
        "LEARNING_RATE = 1e-05\n",
        "num_classes = len(news_groups)\n",
        "num_of_batches_per_epoch = len(X_train)//TRAIN_BATCH_SIZE\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sYiO0Vb_oHj"
      },
      "source": [
        "### Convert data into bert consumable format.\n",
        "\n",
        "BERT expects input data in a specific format, with special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP]). \n",
        "\n",
        "We need to tokenize our text into tokens that correspond to BERTâ€™s vocabulary. Bert uses word piece tokenizer and has a vocabulary size of ~30k words.\n",
        "\n",
        "For each tokenized sentence, Bert requires: \n",
        "\n",
        "a. Input ids, a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary.\n",
        "\n",
        "b. Mask ids, a sequence of integers identifying each masked location as bert uses Masked Language Model(MLM).\n",
        "\n",
        "c. Corresonding targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehtq0e_9b0av"
      },
      "source": [
        "class BertDataFormat(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        cur_doc = str(self.data.doc[index])\n",
        "        cur_doc = \" \".join(cur_doc.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            cur_doc,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.labels[index], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "training_set = BertDataFormat(train_df, tokenizer, MAX_LEN)\n",
        "testing_set = BertDataFormat(test_df, tokenizer, MAX_LEN)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwdMVLK4bIJJ"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr5dzgBTCqmC"
      },
      "source": [
        "### Baseline distil-bert model.\n",
        "\n",
        "For baseline model, I used default pretrained distil-bert model from hugging face and tried to classify the data without any fine tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ouVRC9dKjKj"
      },
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class DistillBERTClass(torch.nn.Module):\n",
        "    def __init__(self,num_classes):\n",
        "        super(DistillBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.classifier = torch.nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        bert_last = hidden_state[:, 0]\n",
        "        output = self.classifier(bert_last)\n",
        "        return output"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD5PvdCCKkR5"
      },
      "source": [
        "# Copy model to device.\n",
        "baseline_model = DistillBERTClass(num_classes)\n",
        "baseline_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA6Ig7o9KniL"
      },
      "source": [
        "# Create the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  baseline_model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwxD_1FVa37D"
      },
      "source": [
        "# Calcuate accuracy of the model\n",
        "def acc_cal(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81NLQbVAa45Q"
      },
      "source": [
        "# train model\n",
        "def train(epoch,model):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "\n",
        "    # progress bar\n",
        "    train_per_epoch = num_of_batches_per_epoch\n",
        "    kbar = pkbar.Kbar(target=train_per_epoch, epoch=epoch, \n",
        "                      num_epochs=EPOCHS, width=8, \n",
        "                      always_stateful=False)\n",
        "\n",
        "    for idx,data in enumerate(training_loader, 0):\n",
        "\n",
        "        # copy tensors to gpu\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        # get output and calculate loss.\n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += acc_cal(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "      \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "        kbar.update(idx, values=[(\"train_loss\", tr_loss/(idx+1))])\n",
        "\n",
        "\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL_YS423y0Q9"
      },
      "source": [
        "# function to predict output.\n",
        "def valid(model, testing_loader):\n",
        "    predicted_labels = []\n",
        "    true_labels = []\n",
        "    nb_tr_steps = 0\n",
        "    tr_loss =0\n",
        "    nb_tr_examples=0\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "\n",
        "            # copy tensors to gpu.\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask).squeeze()\n",
        "\n",
        "            # calculate loss\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            predicted_labels += big_idx\n",
        "            true_labels += targets\n",
        "            \n",
        "            n_correct += acc_cal(big_idx, targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "            \n",
        "    \n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    return epoch_accu,epoch_loss,predicted_labels,true_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM34C6rza-RA",
        "outputId": "753f8a2b-527d-4f00-be39-1400dfd65be0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        }
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch,baseline_model)\n",
        "    print('\\n')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/8\n",
            "1020/1020 [========] - 333s 326ms/step - train_loss: 1.6494\n",
            "Training Loss Epoch: 1.151628428075853\n",
            "Training Accuracy Epoch: 65.61562480866957\n",
            "\n",
            "\n",
            "Epoch: 2/8\n",
            "1020/1020 [========] - 333s 326ms/step - train_loss: 0.6246\n",
            "Training Loss Epoch: 0.6041948671665236\n",
            "Training Accuracy Epoch: 80.38327312802302\n",
            "\n",
            "\n",
            "Epoch: 3/8\n",
            "1020/1020 [========] - 333s 327ms/step - train_loss: 0.4302\n",
            "Training Loss Epoch: 0.4213431019967205\n",
            "Training Accuracy Epoch: 86.64666625849507\n",
            "\n",
            "\n",
            "Epoch: 4/8\n",
            "1020/1020 [========] - 333s 327ms/step - train_loss: 0.3080\n",
            "Training Loss Epoch: 0.3093200043631931\n",
            "Training Accuracy Epoch: 90.06918508540991\n",
            "\n",
            "\n",
            "Epoch: 5/8\n",
            "1020/1020 [========] - 334s 327ms/step - train_loss: 0.2236\n",
            "Training Loss Epoch: 0.2327240092121612\n",
            "Training Accuracy Epoch: 92.38351803098023\n",
            "\n",
            "\n",
            "Epoch: 6/8\n",
            "1020/1020 [========] - 334s 327ms/step - train_loss: 0.1753\n",
            "Training Loss Epoch: 0.18626880633346482\n",
            "Training Accuracy Epoch: 93.61415539092634\n",
            "\n",
            "\n",
            "Epoch: 7/8\n",
            "1020/1020 [========] - 334s 327ms/step - train_loss: 0.1526\n",
            "Training Loss Epoch: 0.15773281787798524\n",
            "Training Accuracy Epoch: 94.36110941039612\n",
            "\n",
            "\n",
            "Epoch: 8/8\n",
            "1020/1020 [========] - 334s 328ms/step - train_loss: 0.1307\n",
            "Training Loss Epoch: 0.1369942877634112\n",
            "Training Accuracy Epoch: 94.93663135982366\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "truOb3XpXXTu",
        "outputId": "97bf0686-82b1-4090-e2bb-d4f6216f21c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "acc,loss,predicted_labels,true_labels = valid(baseline_model, testing_loader)\n",
        "print(\"test accuracy on baseline distilbert model =\",round(acc,2))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy on baseline distilbert model = 83.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra91sNqA6WK0",
        "outputId": "91074fcd-2b38-4400-9e86-b0d753cfed02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted_labels = [i.item() for i in predicted_labels]\n",
        "true_labels = [i.item() for i in true_labels]\n",
        "baseline_f1 = f1_score(true_labels,predicted_labels,average='macro')\n",
        "print(\"F1 score on baseline model = \",baseline_f1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on baseline model =  0.8346094307162266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgGS-mysDtzk"
      },
      "source": [
        "Classification report of baseline bert model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uncLQ4W-__Yh",
        "outputId": "38358509-70f9-44ce-cd5a-79b511abb702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "print(classification_report(true_labels,predicted_labels,target_names=news_groups))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.72      0.64      0.68       203\n",
            "            misc.forsale       0.87      0.77      0.82       202\n",
            "      talk.religion.misc       0.53      0.55      0.54       225\n",
            "                 sci.med       0.88      0.94      0.91       200\n",
            "           comp.graphics       0.88      0.70      0.78       202\n",
            "      talk.politics.guns       0.83      0.81      0.82       211\n",
            "      talk.politics.misc       0.66      0.68      0.67       234\n",
            "        rec.sport.hockey       0.98      0.98      0.98       200\n",
            "          comp.windows.x       0.86      0.91      0.89       200\n",
            "               sci.crypt       0.92      0.92      0.92       200\n",
            "  soc.religion.christian       0.89      0.86      0.88       199\n",
            "   talk.politics.mideast       0.85      0.86      0.85       201\n",
            " comp.os.ms-windows.misc       0.72      0.87      0.79       200\n",
            "comp.sys.ibm.pc.hardware       0.76      0.77      0.76       203\n",
            "         rec.motorcycles       0.95      0.95      0.95       200\n",
            "         sci.electronics       0.83      0.84      0.83       202\n",
            "      rec.sport.baseball       0.99      0.94      0.96       200\n",
            "               rec.autos       0.90      0.91      0.90       201\n",
            "               sci.space       0.88      0.94      0.91       200\n",
            "   comp.sys.mac.hardware       0.86      0.86      0.86       201\n",
            "\n",
            "                accuracy                           0.83      4084\n",
            "               macro avg       0.84      0.83      0.83      4084\n",
            "            weighted avg       0.83      0.83      0.83      4084\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SySoJimD11G"
      },
      "source": [
        "### Model 2: \n",
        "\n",
        "I am inspired by DocBERT: BERT for Document Classification architecture developed by [Adhikari et al., 2019](https://arxiv.org/pdf/1904.08398.pdf) which tries to distil knowledge from bert to LSTMs which are proven to be effective due to long text format of documents. DocBert is current state of the art solution for document classification on Multiple datasets like Reuters dataset. So I implemented DocBert architecture and tweaked it on 20 Newsgroup dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzYChNd73MvQ"
      },
      "source": [
        "\n",
        "class DocBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DocBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.dimension = 64\n",
        "        #self.lstm = nn.LSTM(input_size=768, hidden_size=dimension,num_layers=1,\n",
        "        #                    bidirectional=True)\n",
        "        hidden_dim = 64 \n",
        "        embed_dim = 768\n",
        "\n",
        "        self.lstm  = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.pre_classifier = torch.nn.Linear(64, 64)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(64, 20)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        bert_final = hidden_state[:, 0]\n",
        "        bert_final = bert_final.squeeze().unsqueeze(dim=0)\n",
        "        lstm1, (h, c) = self.lstm(bert_final)\n",
        "        #print(lstm1.shape)\n",
        "        lstm1 = lstm1.view((lstm1.shape)[1],64)\n",
        "        linear1 = self.pre_classifier(lstm1)\n",
        "        linear1 = torch.nn.ReLU()(linear1)\n",
        "        linear1 = self.dropout(linear1)\n",
        "        #print(linear1.shape)\n",
        "        output = self.classifier(linear1)\n",
        "        #print(output.shape)\n",
        "        return output"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZRk8i9RW_Gc",
        "outputId": "1df170d2-dc1c-4023-a0b1-71383c8d705e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Copy model to device.\n",
        "docbert_model = DocBERTClass()\n",
        "docbert_model.to(device)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DocBERTClass(\n",
              "  (l1): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lstm): LSTM(768, 64, batch_first=True)\n",
              "  (pre_classifier): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (classifier): Linear(in_features=64, out_features=20, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5hCUyr2XTfz"
      },
      "source": [
        "# Create the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  docbert_model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT_mOCD-W_Em",
        "outputId": "21c68f14-231a-40b3-e36b-998afbd7b2d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('DocBert Model')\n",
        "EPOCHS = 13\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch,docbert_model)\n",
        "    print('\\n')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DocBert Model\n",
            "Epoch: 1/13\n",
            "1020/1020 [========] - 336s 330ms/step - train_loss: 2.0910\n",
            "Training Loss Epoch: 2.024186410352836\n",
            "Training Accuracy Epoch: 68.86671156554215\n",
            "\n",
            "\n",
            "Epoch: 2/13\n",
            "1020/1020 [========] - 336s 330ms/step - train_loss: 1.7914\n",
            "Training Loss Epoch: 1.7390003384152075\n",
            "Training Accuracy Epoch: 76.64238045674402\n",
            "\n",
            "\n",
            "Epoch: 3/13\n",
            "1020/1020 [========] - 336s 330ms/step - train_loss: 1.5413\n",
            "Training Loss Epoch: 1.495943967205537\n",
            "Training Accuracy Epoch: 81.17308516500337\n",
            "\n",
            "\n",
            "Epoch: 4/13\n",
            "1020/1020 [========] - 336s 330ms/step - train_loss: 1.3253\n",
            "Training Loss Epoch: 1.2848184296129732\n",
            "Training Accuracy Epoch: 84.26498499969387\n",
            "\n",
            "\n",
            "Epoch: 5/13\n",
            "1020/1020 [========] - 336s 329ms/step - train_loss: 1.1388\n",
            "Training Loss Epoch: 1.098104508044554\n",
            "Training Accuracy Epoch: 86.71401457172595\n",
            "\n",
            "\n",
            "Epoch: 6/13\n",
            "1020/1020 [========] - 336s 329ms/step - train_loss: 0.9588\n",
            "Training Loss Epoch: 0.945668080994945\n",
            "Training Accuracy Epoch: 88.13445172350455\n",
            "\n",
            "\n",
            "Epoch: 7/13\n",
            "1020/1020 [========] - 336s 330ms/step - train_loss: 0.8459\n",
            "Training Loss Epoch: 0.816197077803\n",
            "Training Accuracy Epoch: 89.67734035388477\n",
            "\n",
            "\n",
            "Epoch: 8/13\n",
            "1020/1020 [========] - 333s 326ms/step - train_loss: 0.7385\n",
            "Training Loss Epoch: 0.7088179806946073\n",
            "Training Accuracy Epoch: 90.96308087920161\n",
            "\n",
            "\n",
            "Epoch: 9/13\n",
            "1020/1020 [========] - 330s 324ms/step - train_loss: 0.6297\n",
            "Training Loss Epoch: 0.6139628174664575\n",
            "Training Accuracy Epoch: 91.7712606379722\n",
            "\n",
            "\n",
            "Epoch: 10/13\n",
            "1020/1020 [========] - 336s 330ms/step - train_loss: 0.5490\n",
            "Training Loss Epoch: 0.5396569252276864\n",
            "Training Accuracy Epoch: 92.51821465744199\n",
            "\n",
            "\n",
            "Epoch: 11/13\n",
            "1020/1020 [========] - 337s 330ms/step - train_loss: 0.4997\n",
            "Training Loss Epoch: 0.48588770946139337\n",
            "Training Accuracy Epoch: 93.12434947651992\n",
            "\n",
            "\n",
            "Epoch: 12/13\n",
            "1020/1020 [========] - 336s 330ms/step - train_loss: 0.4342\n",
            "Training Loss Epoch: 0.43017267061435743\n",
            "Training Accuracy Epoch: 93.58354252127594\n",
            "\n",
            "\n",
            "Epoch: 13/13\n",
            "1020/1020 [========] - 336s 330ms/step - train_loss: 0.3933\n",
            "Training Loss Epoch: 0.39255604254452187\n",
            "Training Accuracy Epoch: 93.73048429559788\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggFvcUVcZw0f",
        "outputId": "6205b823-483f-47c8-8c8c-0037638e484e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "acc,loss,predicted_labels,true_labels = valid(docbert_model, testing_loader)\n",
        "print(\"test accuracy on DocBert model =\",acc, '%')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy on DocBert model = 88.54064642507346 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGD7zY2uZ5_4",
        "outputId": "6f15db2d-e8f4-4b90-a6e8-a5f85b4dde8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted_labels = [i.item() for i in predicted_labels]\n",
        "true_labels = [i.item() for i in true_labels]\n",
        "docbert_f1 = f1_score(true_labels,predicted_labels,average='macro')\n",
        "print(\"F1 score on docbert model = \",docbert_f1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on docbert model =  0.8970506268868808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlDuMUdaaAzU"
      },
      "source": [
        "### Classification report on DocBert architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52DrUB1FZ67u",
        "outputId": "f11a3fb9-de82-43ba-8742-a2efb557fa16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "print(classification_report(true_labels,predicted_labels,target_names=news_groups))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.84      0.63      0.72       203\n",
            "            misc.forsale       0.88      0.97      0.92       202\n",
            "      talk.religion.misc       0.66      0.60      0.62       225\n",
            "                 sci.med       0.95      0.97      0.96       200\n",
            "           comp.graphics       0.91      0.85      0.88       202\n",
            "      talk.politics.guns       0.81      0.90      0.85       211\n",
            "      talk.politics.misc       0.74      0.75      0.75       234\n",
            "        rec.sport.hockey       1.00      0.99      1.00       200\n",
            "          comp.windows.x       0.95      0.94      0.95       200\n",
            "               sci.crypt       0.98      0.95      0.97       200\n",
            "  soc.religion.christian       0.86      0.98      0.92       199\n",
            "   talk.politics.mideast       0.87      0.95      0.91       201\n",
            " comp.os.ms-windows.misc       0.89      0.87      0.88       200\n",
            "comp.sys.ibm.pc.hardware       0.87      0.78      0.82       203\n",
            "         rec.motorcycles       0.99      0.97      0.98       200\n",
            "         sci.electronics       0.89      0.88      0.88       202\n",
            "      rec.sport.baseball       0.98      0.99      0.99       200\n",
            "               rec.autos       0.96      0.96      0.96       201\n",
            "               sci.space       0.96      0.95      0.96       200\n",
            "   comp.sys.mac.hardware       0.80      0.91      0.85       201\n",
            "\n",
            "                accuracy                           0.89      4084\n",
            "               macro avg       0.89      0.89      0.89      4084\n",
            "            weighted avg       0.89      0.89      0.89      4084\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89T1Aq_n7T_7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}